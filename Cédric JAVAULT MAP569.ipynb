{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet MAP569 Cédric JAVAULT (Master IA Year 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Préliminaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime, date, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE, ADASYN\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import f1_score,accuracy_score,confusion_matrix,precision_score,recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lire le fichier et organiser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier de data comporte 5380 lignes (+ l'entete) et 19 colonnes. La première étape est de charger ces données et surtout de les organiser de manière pertinente pour la suite. Parmi ces 19 colonnes, il y a un Id_Customer qui n'a aucune raison d'être pertinent pour la suite. Je n'intégre évidemment pas le Y dans les données d'entrée. En revanche, je garde toutes les autres colonnes et ne fais **pas de feature selection** : il y a relativement peu de données et mon petit PC va y arriver sans qu'il soit besoin de simplifier le problème. **Cela fait donc 17 colonnes à organiser et une matrice 5380*17 à créer.** \n",
    "\n",
    "\n",
    "Si certaines colonnes sont déjà dans un format numérique pertinent, **d'autres doivent être reformatées intelligemment** car il s'agit de string. Comme j'ai en tête d'utiliser des algorithmes assez simples (et pas un NN qui pourrait faire le job), il faut autant que possible transformer les chaînes de caractères en nombres qui sont pertinents pour ces algorithmes. Typiquement, pour les niveaux d'éducation, il semble pertinent de poser par exemple : 0 pour 'Secondary or lower', 1 pour University -> 1, 2 pour Diploma et 3 pour Master/phD (même si je ne suis pas sûr de comprendre ce que Diploma veut dire pour la banque)...\n",
    "\n",
    "\n",
    "NB : je ne maîtrise pas très bien toutes les subtilités de Python. Plutôt que de passer des heures à trouver la parfaite formulation en Pyton, j'ai gagné du temps en modifiant le fichier initial (sous Excel) pour le réenregistrer avec des ; comme séparateurs et des . pour indiquer les parties décimales (sinon, mon csv.reader confondait le séparateur de champ et celui indiquant la partie décimale). J'ai aussi approximé la date par 365*(année-1900)+30*mois+jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and organise the data\n",
    "index=0\n",
    "nbdata=5381\n",
    "X=np.zeros((nbdata,17))\n",
    "Y=np.zeros(nbdata)\n",
    "\n",
    "with open('CreditTraining2.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';', quoting=csv.QUOTE_NONE)\n",
    "    next(spamreader)\n",
    "   \n",
    "    for row in spamreader:\n",
    "        Y[index]=row[1]\n",
    "\n",
    "        if (row[2]=='Non Existing Client'):\n",
    "            X[index,0]=1 # Sinon on est déjà à 0\n",
    "            \n",
    "        date = datetime.strptime(row[3], '%d/%m/%Y').date() #Bith date\n",
    "        X[index,1]=date.day+date.month*30+(date.year-1900)*365 # Approximation qui suffira \n",
    "        \n",
    "        date= datetime.strptime(row[4], '%d/%m/%Y').date() # Customer_Open_Date\n",
    "        X[index,2]=date.day+date.month*30+(date.year-1900)*365 \n",
    "        \n",
    "        if (row[5]=='NP_Client'):\n",
    "            X[index,3]=1 #Sinon on est déjà à 0\n",
    "\n",
    "        if (row[6]=='University'): # Cas Secondary or less reste à 0\n",
    "            X[index,4]=1 \n",
    "        if (row[6]=='Diploma'):\n",
    "            X[index,4]=2 \n",
    "        if (row[6]=='Master/PhD'):\n",
    "            X[index,4]=3 \n",
    "            \n",
    "        if (row[7]=='Separated'):   # Cas Divorced reste à 0. Intuitivement c'est le pire pour la banque \n",
    "            X[index,5]=1           # Il peut y avoir une pension alimentaire...\n",
    "        if (row[7]=='Widowed'):\n",
    "            X[index,5]=2 \n",
    "        if (row[7]=='Single'):\n",
    "            X[index,5]=3 \n",
    "        if (row[7]=='Married'):     # Meilleure situation à mon sens - En tous cas la plus éloignée de Divorced\n",
    "            X[index,5]=4    \n",
    "        \n",
    "        if (row[8]!='') :\n",
    "            X[index,6]=row[8] # Number_Of_Dependant - Attention deux lignes vides qui resteront donc à zéro\n",
    "            \n",
    "        X[index,7]=row[9] # Years_At_Residence\n",
    "        \n",
    "        if (row[10]!=''):\n",
    "            X[index,8]=float(row[10].replace(',','.')) # Net_Annual_Income - Attention deux lignes vides qui resteront donc à zéro\n",
    "        \n",
    "        if (row[11]!=''):\n",
    "            X[index,9]=row[11] # Years_At_Business - Attention deux lignes vides qui resteront donc à zéro\n",
    "        \n",
    "        if (row[12]=='P'):   # Le C reste à 0\n",
    "            X[index,10]=1\n",
    "        if (row[12]=='G'):    \n",
    "            X[index,10]=2   \n",
    "            \n",
    "        date= datetime.strptime(row[13], '%d/%m/%Y').date() # Years_At_Business\n",
    "        X[index,11]=date.day+date.month*30+(date.year-1900)*365 \n",
    "        \n",
    "        if (row[14]=='Sales'):   # Le Branch reste à 0\n",
    "            X[index,12]=1\n",
    "        \n",
    "        if (row[15]=='New rent'):\n",
    "            X[index,13]=1\n",
    "        if (row[15]=='Old rent'):\n",
    "            X[index,13]=2 \n",
    "        if (row[15]=='Company'):\n",
    "            X[index,13]=3 \n",
    "        if (row[15]=='Owned'):     \n",
    "            X[index,13]=4           \n",
    "        \n",
    "        X[index,14]=row[16] # Nb_Of_Products\n",
    "\n",
    "        if (row[17]!=''):     # Prod_Closed_Date - une majorité de lignes vides\n",
    "            date= datetime.strptime(row[17], '%d/%m/%Y').date() # Years_At_Business\n",
    "            X[index,15]=date.day+date.month*30+(date.year-1900)*365 \n",
    "        else :\n",
    "            X[index,15]=114*365+1 # 1er janvier 2014 soit après toutes les autres dates  \n",
    "         \n",
    "        X[index,16]=ord(row[18])    \n",
    "        \n",
    "        index=index+1        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien sûr, j'ai fait quelques print pour vérifier que les datas étaient correctement chargées. C'est le cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Posons nous un peu sur ce problème !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème proposé est de modéliser pour savoir s'il faut accorder ou non un prêt et plus précisément si l'emprunteur va faire défaut ou pas. Il s'agit d'un problème de classification binaire mais **avec une très forte proportion de Y=0 (92.7%)** représentant les clients qui ne font pas défaut et remboursent bien leur prêt. Il y a à ce stade **quatre remarques importantes** :\n",
    "\n",
    "- Nous sommes dans une situation où **il faut éviter d'attribuer un prêt à une personne qui ferait défaut** ; c'est beaucoup plus grave que d'en refuser un à une personne qui, en fait, rembourserait. Autrement dit, le problème, c'est le cas Ypred=0 alors que Yreal=1 soit les faux négatif (FN). Concrètement, si l'algorithme est utilisé en production, la banque va refuser de prêter si Ypred=1 ou on aura un taux de défaut de FN/(FN+TN). Les FP sont moins grave mais ils posent aussi problème en privant la banque d'une source de revenu. Une métrique pertinente de la qualité de l'algorithme pourrait donc pas exemple de **minimiser (FP+3*FN)/(FP+FN+TP+TN)**. Comme mon facteur 3 est purement empirique et que cette métrique n'est pas commune, **je compare aussi le score  F1=2xPrecisionxRecall/(Precision+Recall)=2TP/(2TP+FN+FP)**. J'affiche enfin le taux de chiffre d'affaire perdu \"inutilement\" et l'amélioration du taux de défaut par rapport à la situation où l'algorithme ne serait pas utilisé.\n",
    "\n",
    "\n",
    "- **Si** (comme cela semble très probable) **les données réelles ont été produites par des agents de la banque, elles sont assez imparfaites** et il sera difficile d'atteindre de très bons scores à partir de ces données.\n",
    "\n",
    "\n",
    "- **L'échantillon est fort réduit surtout sur les Y=1 (seulement 393 cas)**. On va donc avoir des effet de fluctuation statisitique.\n",
    "\n",
    "\n",
    "- Cela sort du cadre de l'exercice mais **nous travaillons sur des données du passé** (et forcément un passé un peu ancien pour savoir si le client a fait ou non défaut). On doit donc supposer que le comportement des clients dans le futur est le même que dans le passé... et qu'aucune grosse crise économique ne va complètement changer les choses !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je procède en 3 étapes pour la suite :\n",
    "\n",
    "- **Je coupe le dataset en un échantillon de training et un autre de test.** Je garde 35% pour la validation finale. Avec 20% et 25%, mes résultats étaient moins bons (overfitting ?).\n",
    "\n",
    "\n",
    "- **Je fais de l'oversampling de la classe Y=1 pour le training set.** J'ai également essayé de faire fonctionner Random_Forest sans ce resampling mais en ajustant les poids comme suggéré en cours (class_weight='balanced' en paramètre) : les résultats sont meilleurs avec ces poids que sans mais moins bon qu'en faisant de l'oversampling. J'ai testé trois algorithmes de resampling et gardé celui qui me donnait les meilleurs résultats.\n",
    "\n",
    "\n",
    "- **Je Teste différents classifier et sélectionne le meilleur en comparant les scores de validation** La Random Forest donne des résultats assez nettement meilleurs que les autres et il n'y a pas d'hyperparamètre à régler (on peut juste jouer sur le nombre d'arbre ; plus il y a d'arbres et meilleure est la prévision ... aux fluctuations aléatoires prêt). **Je ne coupe donc pas mon training set en deux pour avoir un validation set** et utilise directement le testing set pour comparer les modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fabriquer les ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Training and Testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.35, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07091792965398913\n",
      "0.07696390658174097\n"
     ]
    }
   ],
   "source": [
    "#Check that there is roughly the same proportion of Y=1 in both samples\n",
    "print(y_train.mean())\n",
    "print(y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6498, 17)\n",
      "(6498,)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Ressample Trainig set - I tried 3 algorithms and kept the sampliest one\n",
    "X_resampled, y_resampled = RandomOverSampler(random_state=42).fit_resample(X_train, y_train)\n",
    "#X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "#X_resampled, y_resampled = ADASYN().fit_resample(X_train, y_train)\n",
    "\n",
    "# and check\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)\n",
    "print(y_resampled.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Essai de différents classifiers et comparaison des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Petite fonction pour indiquer les résultats\n",
    "def get_result(y):\n",
    "    print(\"Confusion matrix :\")\n",
    "    print(confusion_matrix(y_test, y))\n",
    "    FP=confusion_matrix(y_test, y)[0][1]\n",
    "    FN=confusion_matrix(y_test, y)[1][0]\n",
    "    TP=confusion_matrix(y_test, y)[1][1]\n",
    "    TN=confusion_matrix(y_test, y)[0][0]\n",
    "    print(\"Taux de chiffre d'affaire perdu sans raison :\",FP/(TN+FP))\n",
    "    print(\"Le taux de défaut devrait être :\",FN/(TN+FN),\" - Avant il était de \",(FN+TP)/((FP+FN+TP+TN)))\n",
    "    print(\"Score maison à minimiser :\",(FP+3*FN)/(FP+FN+TP+TN))\n",
    "    print(\"score F1 : \",f1_score(y_test, y))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESSAI avec n_neighbors= 3\n",
      "Confusion matrix :\n",
      "[[1594  145]\n",
      " [  62   83]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.08338125359401956\n",
      "Le taux de défaut devrait être : 0.03743961352657005  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.1756900212314225\n",
      "score F1 :  0.4450402144772118\n",
      "\n",
      "ESSAI avec n_neighbors= 9\n",
      "Confusion matrix :\n",
      "[[1493  246]\n",
      " [  26  119]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.14146060954571593\n",
      "Le taux de défaut devrait être : 0.017116524028966424  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.17197452229299362\n",
      "score F1 :  0.4666666666666667\n",
      "\n",
      "ESSAI avec n_neighbors= 51\n",
      "Confusion matrix :\n",
      "[[1507  232]\n",
      " [  22  123]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.13341000575043127\n",
      "Le taux de défaut devrait être : 0.014388489208633094  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.15817409766454352\n",
      "score F1 :  0.4919999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN - Je ne mets pas tous les résultats, ils sont décevants\n",
    "for n_neighbors in [3,9,51]:\n",
    "    print(\"ESSAI avec n_neighbors=\",n_neighbors)\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_resampled, y_resampled) \n",
    "    ypred_nn=knn.predict(X_test)\n",
    "    get_result(ypred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESSAI AVEC n_estimators= 7\n",
      "Confusion matrix :\n",
      "[[1565  174]\n",
      " [  10  135]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.10005750431282347\n",
      "Le taux de défaut devrait être : 0.006349206349206349  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.10828025477707007\n",
      "score F1 :  0.5947136563876653\n",
      "\n",
      "ESSAI AVEC n_estimators= 23\n",
      "Confusion matrix :\n",
      "[[1574  165]\n",
      " [  11  134]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.09488211615871191\n",
      "Le taux de défaut devrait être : 0.00694006309148265  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.10509554140127389\n",
      "score F1 :  0.6036036036036035\n",
      "\n",
      "ESSAI AVEC n_estimators= 45\n",
      "Confusion matrix :\n",
      "[[1583  156]\n",
      " [  14  131]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.08970672800460035\n",
      "Le taux de défaut devrait être : 0.008766437069505322  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.10509554140127389\n",
      "score F1 :  0.6064814814814815\n",
      "\n",
      "ESSAI AVEC n_estimators= 101\n",
      "Confusion matrix :\n",
      "[[1606  133]\n",
      " [  18  127]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.07648073605520414\n",
      "Le taux de défaut devrait être : 0.011083743842364532  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.09925690021231423\n",
      "score F1 :  0.6271604938271604\n",
      "\n",
      "ESSAI AVEC n_estimators= 303\n",
      "Confusion matrix :\n",
      "[[1623  116]\n",
      " [  19  126]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.06670500287521564\n",
      "Le taux de défaut devrait être : 0.011571254567600487  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.09182590233545647\n",
      "score F1 :  0.6511627906976744\n",
      "\n",
      "ESSAI AVEC n_estimators= 1000\n",
      "Confusion matrix :\n",
      "[[1646   93]\n",
      " [  27  118]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.053479010925819435\n",
      "Le taux de défaut devrait être : 0.016138673042438732  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.09235668789808917\n",
      "score F1 :  0.6629213483146066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Essai de AdaBoost\n",
    "for n_estimators in [7,23,45,101,303,1000]:\n",
    "    print (\"ESSAI AVEC n_estimators=\",n_estimators)\n",
    "    clf = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    clf.fit(X_resampled, y_resampled)\n",
    "    ypred_ada = clf.predict(X_test)\n",
    "    get_result(ypred_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESSAI AVEC Random_state= 43\n",
      "Confusion matrix :\n",
      "[[1728   11]\n",
      " [  75   70]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.0063254744105807935\n",
      "Le taux de défaut devrait être : 0.04159733777038269  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.12526539278131635\n",
      "score F1 :  0.6194690265486726\n",
      "\n",
      "ESSAI AVEC Random_state= 44\n",
      "Confusion matrix :\n",
      "[[1724   15]\n",
      " [  71   74]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.008625646923519264\n",
      "Le taux de défaut devrait être : 0.03955431754874652  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.12101910828025478\n",
      "score F1 :  0.6324786324786326\n",
      "\n",
      "ESSAI AVEC Random_state= 45\n",
      "Confusion matrix :\n",
      "[[1726   13]\n",
      " [  70   75]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.007475560667050029\n",
      "Le taux de défaut devrait être : 0.03897550111358575  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.1183651804670913\n",
      "score F1 :  0.6437768240343348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try Random Forest without using resampling \n",
    "for random_state in range (43,46):\n",
    "    print (\"ESSAI AVEC Random_state=\",random_state)\n",
    "    rf = RandomForestClassifier(n_estimators = 100, random_state = random_state,class_weight='balanced')\n",
    "    rf.fit(X_train, y_train)\n",
    "    ypred_rfnr = rf.predict(X_test)\n",
    "    get_result(ypred_rfnr)\n",
    "    \n",
    "# A OBSERVER : LA FORTE VARIABILITE EN FONCTION DU Random_state !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESSAI AVEC Random_state= 43\n",
      "Confusion matrix :\n",
      "[[1709   30]\n",
      " [  46   99]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.017251293847038527\n",
      "Le taux de défaut devrait être : 0.02621082621082621  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.08917197452229299\n",
      "score F1 :  0.7226277372262774\n",
      "\n",
      "ESSAI AVEC Random_state= 44\n",
      "Confusion matrix :\n",
      "[[1707   32]\n",
      " [  45  100]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.018401380103507763\n",
      "Le taux de défaut devrait être : 0.025684931506849314  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.08864118895966029\n",
      "score F1 :  0.7220216606498194\n",
      "\n",
      "ESSAI AVEC Random_state= 45\n",
      "Confusion matrix :\n",
      "[[1708   31]\n",
      " [  43  102]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.017826336975273145\n",
      "Le taux de défaut devrait être : 0.024557395773843516  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.08492569002123142\n",
      "score F1 :  0.7338129496402878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try Random Forest with resampling : better\n",
    "for random_state in range (43,46):\n",
    "    print (\"ESSAI AVEC Random_state=\",random_state)\n",
    "    rf = RandomForestClassifier(n_estimators = 100, random_state = random_state)\n",
    "    rf.fit(X_resampled, y_resampled)\n",
    "    ypred_rf = rf.predict(X_test)\n",
    "    get_result(ypred_rf)\n",
    "    \n",
    "# IDEM : LA FORTE VARIABILITE EN FONCTION DU Random_state !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESSAI AVEC n_trees= 11\n",
      "Confusion matrix :\n",
      "[[1708   31]\n",
      " [  46   99]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.017826336975273145\n",
      "Le taux de défaut devrait être : 0.026225769669327253  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.08970276008492568\n",
      "score F1 :  0.7200000000000001\n",
      "\n",
      "ESSAI AVEC n_trees= 31\n",
      "Confusion matrix :\n",
      "[[1710   29]\n",
      " [  44  101]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.01667625071880391\n",
      "Le taux de défaut devrait être : 0.02508551881413911  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.08545647558386411\n",
      "score F1 :  0.7345454545454545\n",
      "\n",
      "ESSAI AVEC n_trees= 100\n",
      "Confusion matrix :\n",
      "[[1708   31]\n",
      " [  43  102]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.017826336975273145\n",
      "Le taux de défaut devrait être : 0.024557395773843516  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.08492569002123142\n",
      "score F1 :  0.7338129496402878\n",
      "\n",
      "ESSAI AVEC n_trees= 201\n",
      "Confusion matrix :\n",
      "[[1708   31]\n",
      " [  42  103]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.017826336975273145\n",
      "Le taux de défaut devrait être : 0.024  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.08333333333333333\n",
      "score F1 :  0.7383512544802867\n",
      "\n",
      "ESSAI AVEC n_trees= 301\n",
      "Confusion matrix :\n",
      "[[1710   29]\n",
      " [  43  102]]\n",
      "Taux de chiffre d'affaire perdu sans raison : 0.01667625071880391\n",
      "Le taux de défaut devrait être : 0.02452937820878494  - Avant il était de  0.07696390658174097\n",
      "Score maison à minimiser : 0.08386411889596602\n",
      "score F1 :  0.7391304347826086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Le même en faisant varier le nombre d'arbres\n",
    "random_state=45\n",
    "for n_trees in [11,31,100,201,301]:\n",
    "    print (\"ESSAI AVEC n_trees=\",n_trees)\n",
    "    rf = RandomForestClassifier(n_estimators = n_trees, random_state = random_state)\n",
    "    rf.fit(X_resampled, y_resampled)\n",
    "    ypred_rf = rf.predict(X_test)\n",
    "    get_result(ypred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En gardant 35% de l'échantillon en testing set pour la validation, et en entrainement les 65% après les avoir over samplés (avec RandomOverSampler), j'obtiens mes meilleurs résultats avec **une Foret de 301 arbres : 74% de score F1**. Il faut noter qu'il y a une variation importante due au choix du random_state. **Ce représente une division par plus de 3 du taux de défaut !**\n",
    "\n",
    "Ce niveau autour de 74% n'est pas très élevé mais les données fournies sont assez peu importantes en quantité et, surtout, elles doivent contenir elle-même une part d'erreur : deux agents différents de la même banque, avec les mêmes données, ont probablement des Y qui peuvent être différents (et 74% de F1 correspond tout de même à 96.23% d'accuracy).\n",
    "\n",
    "**IMPORTANT : si le meilleur score F1 est obtenu s'assez loin avec RamdomForest, c'est BEAUCOUP moins clair pour mon 'score maison' qui cherche à minimiser 3*FN + FP. Si on prenait 10*FN+FP, Adaboost serait meilleur car il donne nettement moins de FN (10 à 20 contre 40 à 50 pour RF). D'ailleurs, certains résultats d'Adaboost montrent une division par 10 du taux de défaut (mais avec aussi un fort taux de rejet de dossier qu'il aurait fallu accepter). Pour aller plus loin et vraiment conclure, il faudrait connaître le coût économique d'un défaut du client et le comparer au coût économique d'un prêt non accordé (alors qu'il aurait été remboursé), c'est-à-dire choisir la fonction de perte en fonction de paramètres \"réels\" ; c'est sans doute au delà de cet exercice.**\n",
    "\n",
    "A noter : pour finir, une fois le modèle sélectionné, il faudrait le refaire tourner sur 100% du dataset ce qui devrait un peu améliorer la performance. Mais bien sûr, faute de testing set, on ne pourrait alors qu'estimer sa performance par une cross validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
